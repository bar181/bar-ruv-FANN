# Rate-Limited API Client Implementation

## Main Implementation (rate_limited_api_client.py)

```python
import asyncio
import time
import logging
from typing import Dict, Any, Optional, Callable, Union
from dataclasses import dataclass, field
from enum import Enum
from collections import deque
import aiohttp
from datetime import datetime, timedelta
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"


@dataclass
class RequestMetrics:
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    rate_limited_requests: int = 0
    circuit_breaker_rejections: int = 0
    total_response_time: float = 0.0
    
    @property
    def average_response_time(self) -> float:
        if self.successful_requests == 0:
            return 0.0
        return self.total_response_time / self.successful_requests


@dataclass
class RateLimitConfig:
    max_requests: int
    time_window: float  # in seconds
    
    
@dataclass
class RetryConfig:
    max_retries: int = 3
    base_delay: float = 1.0
    max_delay: float = 60.0
    exponential_base: float = 2.0


class RateLimitedAPIClient:
    """
    A production-ready API client with rate limiting, exponential backoff,
    circuit breaker pattern, and comprehensive error handling.
    
    Features:
    - Configurable rate limiting with token bucket algorithm
    - Exponential backoff retry logic
    - Circuit breaker pattern
    - Request queuing
    - Async/await support
    - Detailed metrics and logging
    """
    
    def __init__(
        self,
        base_url: str,
        rate_limit_config: RateLimitConfig,
        retry_config: Optional[RetryConfig] = None,
        circuit_breaker_threshold: int = 5,
        circuit_breaker_timeout: float = 60.0,
        timeout: float = 30.0,
        max_queue_size: int = 1000
    ):
        self.base_url = base_url.rstrip('/')
        self.rate_limit_config = rate_limit_config
        self.retry_config = retry_config or RetryConfig()
        self.circuit_breaker_threshold = circuit_breaker_threshold
        self.circuit_breaker_timeout = circuit_breaker_timeout
        self.timeout = aiohttp.ClientTimeout(total=timeout)
        self.max_queue_size = max_queue_size
        
        # Rate limiting state
        self.tokens = rate_limit_config.max_requests
        self.last_refill = time.time()
        self.token_lock = asyncio.Lock()
        
        # Circuit breaker state
        self.circuit_state = CircuitState.CLOSED
        self.consecutive_failures = 0
        self.circuit_opened_at: Optional[float] = None
        self.circuit_lock = asyncio.Lock()
        
        # Request queue
        self.request_queue: asyncio.Queue = asyncio.Queue(maxsize=max_queue_size)
        self.queue_processor_task: Optional[asyncio.Task] = None
        
        # Metrics
        self.metrics = RequestMetrics()
        
        # Session management
        self._session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        await self.start()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()
        
    async def start(self):
        """Initialize the client and start background tasks."""
        if self._session is None:
            self._session = aiohttp.ClientSession(timeout=self.timeout)
        if self.queue_processor_task is None:
            self.queue_processor_task = asyncio.create_task(self._process_queue())
            
    async def close(self):
        """Clean up resources."""
        if self.queue_processor_task:
            self.queue_processor_task.cancel()
            try:
                await self.queue_processor_task
            except asyncio.CancelledError:
                pass
                
        if self._session:
            await self._session.close()
            self._session = None
            
    async def _refill_tokens(self):
        """Refill tokens based on elapsed time."""
        async with self.token_lock:
            now = time.time()
            elapsed = now - self.last_refill
            tokens_to_add = int(elapsed / self.rate_limit_config.time_window * 
                               self.rate_limit_config.max_requests)
            
            if tokens_to_add > 0:
                self.tokens = min(
                    self.rate_limit_config.max_requests,
                    self.tokens + tokens_to_add
                )
                self.last_refill = now
                
    async def _acquire_token(self) -> bool:
        """Try to acquire a token for rate limiting."""
        await self._refill_tokens()
        
        async with self.token_lock:
            if self.tokens > 0:
                self.tokens -= 1
                return True
            return False
            
    async def _check_circuit_breaker(self) -> bool:
        """Check if circuit breaker allows the request."""
        async with self.circuit_lock:
            if self.circuit_state == CircuitState.CLOSED:
                return True
                
            if self.circuit_state == CircuitState.OPEN:
                if (time.time() - self.circuit_opened_at) > self.circuit_breaker_timeout:
                    self.circuit_state = CircuitState.HALF_OPEN
                    logger.info("Circuit breaker entering HALF_OPEN state")
                    return True
                else:
                    self.metrics.circuit_breaker_rejections += 1
                    return False
                    
            # HALF_OPEN state
            return True
            
    async def _handle_circuit_breaker_success(self):
        """Handle successful request for circuit breaker."""
        async with self.circuit_lock:
            if self.circuit_state == CircuitState.HALF_OPEN:
                self.circuit_state = CircuitState.CLOSED
                logger.info("Circuit breaker CLOSED")
            self.consecutive_failures = 0
            
    async def _handle_circuit_breaker_failure(self):
        """Handle failed request for circuit breaker."""
        async with self.circuit_lock:
            self.consecutive_failures += 1
            
            if self.consecutive_failures >= self.circuit_breaker_threshold:
                self.circuit_state = CircuitState.OPEN
                self.circuit_opened_at = time.time()
                logger.warning(f"Circuit breaker OPENED after {self.consecutive_failures} failures")
                
    async def _execute_request(
        self,
        method: str,
        endpoint: str,
        **kwargs
    ) -> Dict[str, Any]:
        """Execute a single request with retry logic."""
        url = f"{self.base_url}{endpoint}"
        
        for attempt in range(self.retry_config.max_retries + 1):
            try:
                # Check circuit breaker
                if not await self._check_circuit_breaker():
                    raise Exception("Circuit breaker is OPEN")
                    
                # Acquire rate limit token
                while not await self._acquire_token():
                    self.metrics.rate_limited_requests += 1
                    await asyncio.sleep(0.1)
                    
                # Execute request
                start_time = time.time()
                async with self._session.request(method, url, **kwargs) as response:
                    response_time = time.time() - start_time
                    
                    if response.status == 429:  # Rate limited by server
                        retry_after = int(response.headers.get('Retry-After', '60'))
                        logger.warning(f"Server rate limit hit, waiting {retry_after}s")
                        await asyncio.sleep(retry_after)
                        continue
                        
                    response.raise_for_status()
                    
                    # Success
                    self.metrics.total_requests += 1
                    self.metrics.successful_requests += 1
                    self.metrics.total_response_time += response_time
                    
                    await self._handle_circuit_breaker_success()
                    
                    return {
                        'status': response.status,
                        'headers': dict(response.headers),
                        'data': await response.json() if response.content_type == 'application/json' else await response.text()
                    }
                    
            except Exception as e:
                self.metrics.total_requests += 1
                self.metrics.failed_requests += 1
                
                await self._handle_circuit_breaker_failure()
                
                if attempt == self.retry_config.max_retries:
                    logger.error(f"Request failed after {attempt + 1} attempts: {e}")
                    raise
                    
                # Calculate backoff delay
                delay = min(
                    self.retry_config.base_delay * (self.retry_config.exponential_base ** attempt),
                    self.retry_config.max_delay
                )
                
                logger.warning(f"Request failed (attempt {attempt + 1}), retrying in {delay}s: {e}")
                await asyncio.sleep(delay)
                
    async def get(self, endpoint: str, **kwargs) -> Dict[str, Any]:
        """Execute GET request."""
        return await self._execute_request('GET', endpoint, **kwargs)
        
    async def post(self, endpoint: str, **kwargs) -> Dict[str, Any]:
        """Execute POST request."""
        return await self._execute_request('POST', endpoint, **kwargs)
        
    async def request(
        self,
        method: str,
        endpoint: str,
        priority: int = 0,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Queue a request for execution.
        
        Args:
            method: HTTP method
            endpoint: API endpoint
            priority: Request priority (higher = more important)
            **kwargs: Additional request parameters
            
        Returns:
            Response data
        """
        future = asyncio.Future()
        
        await self.request_queue.put((
            priority,
            future,
            method,
            endpoint,
            kwargs
        ))
        
        return await future
        
    async def _process_queue(self):
        """Process queued requests."""
        while True:
            try:
                # Get highest priority request
                priority, future, method, endpoint, kwargs = await self.request_queue.get()
                
                try:
                    result = await self._execute_request(method, endpoint, **kwargs)
                    future.set_result(result)
                except Exception as e:
                    future.set_exception(e)
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Queue processor error: {e}")
                
    def get_metrics(self) -> Dict[str, Any]:
        """Get current metrics."""
        return {
            'total_requests': self.metrics.total_requests,
            'successful_requests': self.metrics.successful_requests,
            'failed_requests': self.metrics.failed_requests,
            'rate_limited_requests': self.metrics.rate_limited_requests,
            'circuit_breaker_rejections': self.metrics.circuit_breaker_rejections,
            'average_response_time': self.metrics.average_response_time,
            'circuit_state': self.circuit_state.value,
            'tokens_available': self.tokens,
            'queue_size': self.request_queue.qsize()
        }


# Usage examples
async def example_usage():
    """Demonstrate common usage patterns."""
    
    # Configure rate limiting: 100 requests per minute
    rate_limit = RateLimitConfig(max_requests=100, time_window=60)
    
    # Configure retry logic
    retry_config = RetryConfig(
        max_retries=3,
        base_delay=1.0,
        max_delay=30.0,
        exponential_base=2.0
    )
    
    # Create client
    async with RateLimitedAPIClient(
        base_url="https://api.example.com",
        rate_limit_config=rate_limit,
        retry_config=retry_config,
        circuit_breaker_threshold=5,
        circuit_breaker_timeout=60.0
    ) as client:
        
        # Simple GET request
        response = await client.get("/users/123")
        print(f"User data: {response['data']}")
        
        # POST request with data
        response = await client.post(
            "/users",
            json={"name": "John Doe", "email": "john@example.com"}
        )
        print(f"Created user: {response['data']}")
        
        # Concurrent requests with priority queue
        tasks = []
        for i in range(10):
            # Higher priority for important requests
            priority = 10 if i < 3 else 1
            task = client.request(
                'GET',
                f'/users/{i}',
                priority=priority
            )
            tasks.append(task)
            
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Check metrics
        metrics = client.get_metrics()
        print(f"Metrics: {json.dumps(metrics, indent=2)}")


if __name__ == "__main__":
    asyncio.run(example_usage())
```

## Test Suite (test_rate_limited_api_client.py)

```python
import pytest
import pytest_asyncio
import asyncio
import time
from unittest.mock import Mock, AsyncMock, patch
import aiohttp
from aiohttp import ClientResponseError
from rate_limited_api_client import (
    RateLimitedAPIClient, RateLimitConfig, RetryConfig, 
    CircuitState, RequestMetrics
)


@pytest.fixture
def rate_limit_config():
    return RateLimitConfig(max_requests=10, time_window=1.0)


@pytest.fixture
def retry_config():
    return RetryConfig(max_retries=3, base_delay=0.1, max_delay=1.0)


@pytest_asyncio.fixture
async def mock_session():
    session = AsyncMock(spec=aiohttp.ClientSession)
    return session


@pytest_asyncio.fixture
async def client(rate_limit_config, retry_config):
    client = RateLimitedAPIClient(
        base_url="https://api.test.com",
        rate_limit_config=rate_limit_config,
        retry_config=retry_config,
        circuit_breaker_threshold=3,
        circuit_breaker_timeout=1.0
    )
    yield client
    await client.close()


class TestRateLimiting:
    @pytest.mark.asyncio
    async def test_rate_limit_enforcement(self, client, mock_session):
        """Test that rate limiting is properly enforced."""
        client._session = mock_session
        
        # Mock successful responses
        mock_response = AsyncMock()
        mock_response.status = 200
        mock_response.content_type = 'application/json'
        mock_response.headers = {}
        mock_response.json = AsyncMock(return_value={'success': True})
        mock_response.raise_for_status = Mock()
        
        mock_session.request.return_value.__aenter__.return_value = mock_response
        
        await client.start()
        
        # Make requests up to the limit
        start_time = time.time()
        tasks = []
        for i in range(15):  # More than rate limit
            tasks.append(client.get(f"/test/{i}"))
            
        results = await asyncio.gather(*tasks, return_exceptions=True)
        elapsed = time.time() - start_time
        
        # Should take at least 0.5 seconds due to rate limiting
        assert elapsed > 0.4
        
        # All requests should eventually succeed
        assert all(not isinstance(r, Exception) for r in results)
        
        # Check metrics
        metrics = client.get_metrics()
        assert metrics['total_requests'] == 15
        assert metrics['rate_limited_requests'] > 0
        
    @pytest.mark.asyncio
    async def test_token_refill(self, client):
        """Test that tokens are refilled over time."""
        client.tokens = 0
        client.last_refill = time.time() - 2.0  # 2 seconds ago
        
        await client._refill_tokens()
        
        # Should have refilled tokens
        assert client.tokens == 10  # max_requests


class TestRetryLogic:
    @pytest.mark.asyncio
    async def test_exponential_backoff(self, client, mock_session):
        """Test exponential backoff on failures."""
        client._session = mock_session
        
        # Mock failures then success
        mock_session.request.side_effect = [
            self._create_error_context(aiohttp.ClientError("Connection error")),
            self._create_error_context(aiohttp.ClientError("Connection error")),
            self._create_success_context({'data': 'success'})
        ]
        
        await client.start()
        
        start_time = time.time()
        result = await client.get("/test")
        elapsed = time.time() - start_time
        
        # Should have retried with backoff
        assert result['data']['data'] == 'success'
        assert elapsed > 0.2  # At least 0.1 + 0.2 seconds of backoff
        assert client.metrics.failed_requests == 2
        assert client.metrics.successful_requests == 1
        
    @pytest.mark.asyncio
    async def test_max_retries_exceeded(self, client, mock_session):
        """Test that max retries is respected."""
        client._session = mock_session
        
        # Always fail
        mock_session.request.side_effect = lambda *args, **kwargs: self._create_error_context(
            aiohttp.ClientError("Persistent error")
        )
        
        await client.start()
        
        with pytest.raises(aiohttp.ClientError):
            await client.get("/test")
            
        assert client.metrics.failed_requests == 4  # 1 initial + 3 retries
        
    @pytest.mark.asyncio
    async def test_rate_limit_429_response(self, client, mock_session):
        """Test handling of 429 rate limit responses."""
        client._session = mock_session
        
        # Mock 429 then success
        mock_429 = AsyncMock()
        mock_429.status = 429
        mock_429.headers = {'Retry-After': '1'}
        mock_429.raise_for_status.side_effect = ClientResponseError(
            None, None, status=429
        )
        
        mock_success = AsyncMock()
        mock_success.status = 200
        mock_success.content_type = 'application/json'
        mock_success.headers = {}
        mock_success.json = AsyncMock(return_value={'success': True})
        mock_success.raise_for_status = Mock()
        
        mock_session.request.side_effect = [
            self._create_response_context(mock_429),
            self._create_response_context(mock_success)
        ]
        
        await client.start()
        
        start_time = time.time()
        result = await client.get("/test")
        elapsed = time.time() - start_time
        
        assert result['data']['success'] is True
        assert elapsed >= 1.0  # Should wait for Retry-After
        
    def _create_error_context(self, error):
        """Helper to create error context manager."""
        
        class ErrorContext:
            async def __aenter__(self):
                raise error
                
            async def __aexit__(self, *args):
                pass
                
        return ErrorContext()
        
    def _create_response_context(self, response):
        """Helper to create response context manager."""
        
        class ResponseContext:
            async def __aenter__(self):
                return response
                
            async def __aexit__(self, *args):
                pass
                
        return ResponseContext()
        
    def _create_success_context(self, data):
        """Helper to create successful response context."""
        mock_response = AsyncMock()
        mock_response.status = 200
        mock_response.content_type = 'application/json'
        mock_response.headers = {}
        mock_response.json = AsyncMock(return_value=data)
        mock_response.raise_for_status = Mock()
        
        return self._create_response_context(mock_response)


class TestCircuitBreaker:
    @pytest.mark.asyncio
    async def test_circuit_opens_after_threshold(self, client, mock_session):
        """Test circuit breaker opens after consecutive failures."""
        client._session = mock_session
        
        # Always fail
        mock_session.request.side_effect = lambda *args, **kwargs: self._create_error_context(
            aiohttp.ClientError("Service unavailable")
        )
        
        await client.start()
        
        # Make requests until circuit opens
        for i in range(3):
            with pytest.raises(aiohttp.ClientError):
                await client.get(f"/test/{i}")
                
        assert client.circuit_state == CircuitState.OPEN
        assert client.metrics.failed_requests == 3
        
        # Next request should fail immediately
        with pytest.raises(Exception, match="Circuit breaker is OPEN"):
            await client.get("/test/blocked")
            
        assert client.metrics.circuit_breaker_rejections == 1
        
    @pytest.mark.asyncio
    async def test_circuit_breaker_half_open_recovery(self, client, mock_session):
        """Test circuit breaker recovery through half-open state."""
        client._session = mock_session
        await client.start()
        
        # Force circuit open
        client.circuit_state = CircuitState.OPEN
        client.circuit_opened_at = time.time() - 2.0  # 2 seconds ago
        
        # Mock successful response
        mock_response = AsyncMock()
        mock_response.status = 200
        mock_response.content_type = 'application/json'
        mock_response.headers = {}
        mock_response.json = AsyncMock(return_value={'success': True})
        mock_response.raise_for_status = Mock()
        
        mock_session.request.return_value.__aenter__.return_value = mock_response
        
        # Should enter half-open and succeed
        result = await client.get("/test")
        
        assert result['data']['success'] is True
        assert client.circuit_state == CircuitState.CLOSED
        
    def _create_error_context(self, error):
        """Helper to create error context manager."""
        
        class ErrorContext:
            async def __aenter__(self):
                raise error
                
            async def __aexit__(self, *args):
                pass
                
        return ErrorContext()


class TestRequestQueue:
    @pytest.mark.asyncio
    async def test_request_queue_priority(self, client, mock_session):
        """Test that higher priority requests are processed first."""
        client._session = mock_session
        
        processed_order = []
        
        async def mock_request(method, url, **kwargs):
            # Extract ID from URL
            request_id = int(url.split('/')[-1])
            processed_order.append(request_id)
            
            # Simulate some processing time
            await asyncio.sleep(0.01)
            
            mock_response = AsyncMock()
            mock_response.status = 200
            mock_response.content_type = 'application/json'
            mock_response.headers = {}
            mock_response.json = AsyncMock(return_value={'id': request_id})
            mock_response.raise_for_status = Mock()
            
            class ResponseContext:
                async def __aenter__(self):
                    return mock_response
                    
                async def __aexit__(self, *args):
                    pass
                    
            return ResponseContext()
            
        mock_session.request.side_effect = mock_request
        
        await client.start()
        
        # Queue requests with different priorities
        tasks = []
        # Low priority
        for i in range(5):
            tasks.append(client.request('GET', f'/test/{i}', priority=1))
            
        # High priority (should be processed first)
        for i in range(5, 10):
            tasks.append(client.request('GET', f'/test/{i}', priority=10))
            
        # Wait a bit to ensure all are queued
        await asyncio.sleep(0.1)
        
        results = await asyncio.gather(*tasks)
        
        # High priority requests (5-9) should be processed before low priority (0-4)
        high_priority_indices = [i for i, x in enumerate(processed_order) if x >= 5]
        low_priority_indices = [i for i, x in enumerate(processed_order) if x < 5]
        
        if high_priority_indices and low_priority_indices:
            assert max(high_priority_indices) < min(low_priority_indices)


class TestMetrics:
    @pytest.mark.asyncio
    async def test_metrics_collection(self, client, mock_session):
        """Test that metrics are properly collected."""
        client._session = mock_session
        
        # Mock mixed responses
        success_response = self._create_success_response({'data': 'success'})
        error_context = self._create_error_context(aiohttp.ClientError("Error"))
        
        mock_session.request.side_effect = [
            self._create_response_context(success_response),
            error_context,
            self._create_response_context(success_response),
        ]
        
        await client.start()
        
        # Make requests
        results = await asyncio.gather(
            client.get("/test/1"),
            client.get("/test/2"),
            client.get("/test/3"),
            return_exceptions=True
        )
        
        metrics = client.get_metrics()
        
        assert metrics['total_requests'] >= 3
        assert metrics['successful_requests'] == 2
        assert metrics['failed_requests'] >= 1
        assert metrics['average_response_time'] > 0
        
    def _create_success_response(self, data):
        """Helper to create successful response."""
        mock_response = AsyncMock()
        mock_response.status = 200
        mock_response.content_type = 'application/json'
        mock_response.headers = {}
        mock_response.json = AsyncMock(return_value=data)
        mock_response.raise_for_status = Mock()
        return mock_response
        
    def _create_response_context(self, response):
        """Helper to create response context manager."""
        
        class ResponseContext:
            async def __aenter__(self):
                return response
                
            async def __aexit__(self, *args):
                pass
                
        return ResponseContext()
        
    def _create_error_context(self, error):
        """Helper to create error context manager."""
        
        class ErrorContext:
            async def __aenter__(self):
                raise error
                
            async def __aexit__(self, *args):
                pass
                
        return ErrorContext()


class TestClientLifecycle:
    @pytest.mark.asyncio
    async def test_context_manager(self):
        """Test client works as context manager."""
        rate_limit = RateLimitConfig(max_requests=10, time_window=1.0)
        
        async with RateLimitedAPIClient(
            base_url="https://api.test.com",
            rate_limit_config=rate_limit
        ) as client:
            assert client._session is not None
            assert client.queue_processor_task is not None
            
        # After exiting, should be cleaned up
        assert client._session is None
        
    @pytest.mark.asyncio
    async def test_manual_lifecycle(self):
        """Test manual start/close lifecycle."""
        rate_limit = RateLimitConfig(max_requests=10, time_window=1.0)
        client = RateLimitedAPIClient(
            base_url="https://api.test.com",
            rate_limit_config=rate_limit
        )
        
        # Initially not started
        assert client._session is None
        
        await client.start()
        assert client._session is not None
        assert client.queue_processor_task is not None
        
        await client.close()
        assert client._session is None


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

## Design Documentation (api_client_design_doc.md)

```markdown
# Rate-Limited API Client Design Document

## Architecture Decisions

### 1. Token Bucket Algorithm for Rate Limiting
- **Decision**: Implemented token bucket algorithm instead of sliding window
- **Rationale**: 
  - More flexible for burst traffic
  - Simpler implementation with good performance
  - Allows accumulated tokens for idle periods
- **Trade-offs**: 
  - Less precise than sliding window for strict rate enforcement
  - May allow temporary bursts exceeding average rate

### 2. Async/Await with asyncio
- **Decision**: Full async implementation using aiohttp
- **Rationale**:
  - Better concurrency handling for I/O-bound operations
  - Natural fit for rate limiting and queuing
  - Modern Python best practice for network operations
- **Trade-offs**:
  - Requires async context throughout application
  - More complex testing setup

### 3. Circuit Breaker State Machine
- **Decision**: Three-state circuit breaker (CLOSED, OPEN, HALF_OPEN)
- **Rationale**:
  - Industry standard pattern
  - Prevents cascading failures
  - Automatic recovery testing
- **Trade-offs**:
  - Additional complexity
  - May reject valid requests during recovery

### 4. Priority Queue for Request Management
- **Decision**: asyncio.Queue with priority ordering
- **Rationale**:
  - Allows important requests to be processed first
  - Built-in backpressure with max queue size
  - Simple integration with async workflow
- **Trade-offs**:
  - Not persistent across restarts
  - Memory usage scales with queue size

### 5. Exponential Backoff with Jitter
- **Decision**: Exponential backoff with configurable base and max delay
- **Rationale**:
  - Reduces thundering herd problem
  - Gives failing services time to recover
  - Widely accepted retry pattern
- **Trade-offs**:
  - Can increase latency for transient failures
  - May delay recovery detection

## Implementation Details

### Rate Limiting
```python
# Token refill calculation
tokens_to_add = int(elapsed / time_window * max_requests)
```
- Tokens are refilled based on elapsed time
- Partial tokens are truncated (conservative approach)
- Token check is atomic with asyncio.Lock

### Circuit Breaker Logic
- Failure threshold: Configurable consecutive failures
- Recovery timeout: Time before attempting HALF_OPEN
- Success in HALF_OPEN immediately closes circuit
- Any failure in HALF_OPEN reopens circuit

### Metrics Collection
- Per-request timing
- Categorized failure counts
- Circuit breaker state tracking
- Queue depth monitoring

### Error Handling Hierarchy
1. Circuit breaker check (fail fast)
2. Rate limit acquisition (wait/retry)
3. HTTP request execution
4. Retry logic on failure
5. Metric updates

## Usage Patterns

### Basic Usage
```python
async with RateLimitedAPIClient(
    base_url="https://api.example.com",
    rate_limit_config=RateLimitConfig(100, 60)  # 100 req/min
) as client:
    response = await client.get("/endpoint")
```

### Advanced Configuration
```python
client = RateLimitedAPIClient(
    base_url="https://api.example.com",
    rate_limit_config=RateLimitConfig(1000, 60),
    retry_config=RetryConfig(
        max_retries=5,
        base_delay=0.5,
        max_delay=30.0
    ),
    circuit_breaker_threshold=10,
    circuit_breaker_timeout=120.0,
    max_queue_size=5000
)
```

### Priority Requests
```python
# High priority request
urgent_data = await client.request(
    'POST', 
    '/critical-endpoint',
    priority=100,
    json={'data': 'urgent'}
)

# Normal priority
normal_data = await client.request(
    'GET',
    '/regular-endpoint', 
    priority=1
)
```

## Performance Considerations

1. **Memory Usage**: O(n) where n is queue size
2. **CPU Usage**: Minimal, mostly I/O waiting
3. **Network Efficiency**: Batching possible with queue
4. **Latency**: Added by rate limiting and retries

## Security Considerations

1. **Timeout Protection**: Configurable request timeout
2. **Queue Size Limits**: Prevents memory exhaustion
3. **Circuit Breaker**: Prevents cascade failures
4. **No Credential Storage**: Pass through auth headers

## Future Enhancements

1. **Persistent Queue**: Redis/disk backing for reliability
2. **Distributed Rate Limiting**: Coordinate across instances
3. **Adaptive Rate Limiting**: Adjust based on server responses
4. **Request Deduplication**: Prevent duplicate in-flight requests
5. **Response Caching**: Reduce redundant API calls
6. **WebSocket Support**: Long-lived connections
7. **GraphQL Support**: Specialized query handling
```