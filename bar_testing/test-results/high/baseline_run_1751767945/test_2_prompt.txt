Debug and fix the following issues in a distributed task processing system:

The provided code has several critical bugs:
1. Race condition causing duplicate task processing
2. Potential deadlock between worker threads
3. Memory leak in the task queue implementation
4. Incorrect error propagation causing silent failures
5. Task results occasionally being lost or corrupted

Here's the buggy code:

```python
import threading
import queue
import time
import random
from typing import Dict, Any, Optional, Callable
from dataclasses import dataclass
import weakref

@dataclass
class Task:
    id: str
    payload: Dict[str, Any]
    callback: Optional[Callable] = None
    retry_count: int = 0
    max_retries: int = 3

class DistributedTaskProcessor:
    def __init__(self, num_workers: int = 4):
        self.num_workers = num_workers
        self.task_queue = queue.Queue()
        self.result_store = {}
        self.processing_tasks = set()
        self.workers = []
        self.lock = threading.Lock()
        self.shutdown = False
        self._start_workers()
    
    def _start_workers(self):
        for i in range(self.num_workers):
            worker = threading.Thread(target=self._worker_loop, args=(i,))
            worker.start()
            self.workers.append(worker)
    
    def _worker_loop(self, worker_id: int):
        while not self.shutdown:
            try:
                task = self.task_queue.get(timeout=1)
                
                # Bug: Race condition here
                if task.id in self.processing_tasks:
                    self.task_queue.put(task)
                    continue
                
                self.processing_tasks.add(task.id)
                
                # Process task
                result = self._process_task(task)
                
                # Bug: Potential deadlock
                with self.lock:
                    self.result_store[task.id] = result
                    if task.callback:
                        task.callback(result)
                
                # Bug: Memory leak - tasks never removed from processing_tasks
                
            except queue.Empty:
                continue
            except Exception as e:
                # Bug: Error not properly handled
                print(f"Worker {worker_id} error: {e}")
    
    def _process_task(self, task: Task) -> Dict[str, Any]:
        # Simulate processing
        time.sleep(random.uniform(0.1, 0.5))
        
        # Randomly fail some tasks
        if random.random() < 0.1:
            raise Exception("Task processing failed")
        
        return {"task_id": task.id, "result": "processed", "data": task.payload}
    
    def submit_task(self, task: Task) -> str:
        # Bug: No check for duplicate task IDs
        self.task_queue.put(task)
        return task.id
    
    def get_result(self, task_id: str) -> Optional[Dict[str, Any]]:
        # Bug: Not thread-safe
        return self.result_store.get(task_id)
    
    def shutdown_workers(self):
        self.shutdown = True
        for worker in self.workers:
            worker.join()
```

Requirements:
1. Identify and fix ALL bugs
2. Explain the root cause of each issue
3. Provide the corrected implementation
4. Add proper tests to verify the fixes
5. Ensure thread-safety throughout
6. Implement proper cleanup and resource management